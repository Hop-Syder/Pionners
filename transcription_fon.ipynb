{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc90a25d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-08T22:17:32.538343Z",
     "iopub.status.busy": "2025-07-08T22:17:32.538115Z",
     "iopub.status.idle": "2025-07-08T22:17:34.103687Z",
     "shell.execute_reply": "2025-07-08T22:17:34.102888Z"
    },
    "papermill": {
     "duration": 1.570894,
     "end_time": "2025-07-08T22:17:34.105014",
     "exception": false,
     "start_time": "2025-07-08T22:17:32.534120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fon-audios-raw/train-ag-00009-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ac-00005-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-aa-00006-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-p-00004-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-bg-00010-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-s-00003-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-bc-00005-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ah-00010-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ab-00002-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-aq-00009-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-b-00002-of-00003.parquet\n",
      "/kaggle/input/fon-audios-raw/train-i-00005-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ap-00006-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-n-00002-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-k-00004-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-z-00007-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-bf-00011-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-e-00007-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ae-00001-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-a-00001-of-00003.parquet\n",
      "/kaggle/input/fon-audios-raw/train-j-00000-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-u-00008-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-be-00009-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-af-00003-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-an-00003-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-o-00000-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-bd-00008-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-as-00007-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-h-00002-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-d-00008-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-c-00000-of-00003.parquet\n",
      "/kaggle/input/fon-audios-raw/train-l-00009-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ar-00008-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ba-00006-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ax-00000-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-am-00005-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-az-00004-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-g-00003-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-r-00006-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-aw-00003-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ad-00008-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-au-00002-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ao-00004-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-aj-00001-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-w-00007-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-x-00000-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-t-00005-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-f-00001-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-v-00009-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-y-00004-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ai-00011-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-bb-00007-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-at-00010-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ay-00001-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-q-00001-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ak-00000-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-al-00002-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-m-00006-of-00010.parquet\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb03c74d",
   "metadata": {
    "papermill": {
     "duration": 0.002335,
     "end_time": "2025-07-08T22:17:34.110415",
     "exception": false,
     "start_time": "2025-07-08T22:17:34.108080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# Academi Pionners 3 : Test Pipeline Transcription Fon\n",
    "\n",
    "**@hopsyder - Kaggle Free Tier Optimized**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cfa290",
   "metadata": {
    "papermill": {
     "duration": 0.002292,
     "end_time": "2025-07-08T22:17:34.115154",
     "exception": false,
     "start_time": "2025-07-08T22:17:34.112862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ===============================================\n",
    "# INSTALLATION ET CONFIGURATION INITIALE\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536474c2",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2025-07-08T22:17:34.121507Z",
     "iopub.status.busy": "2025-07-08T22:17:34.120941Z",
     "iopub.status.idle": "2025-07-08T22:19:15.539844Z",
     "shell.execute_reply": "2025-07-08T22:19:15.539124Z"
    },
    "papermill": {
     "duration": 101.423503,
     "end_time": "2025-07-08T22:19:15.541080",
     "exception": false,
     "start_time": "2025-07-08T22:17:34.117577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n",
      "Collecting openai-whisper\r\n",
      "  Downloading openai_whisper-20250625.tar.gz (803 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\r\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.26.4)\r\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\r\n",
      "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2.4.1)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.13.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.4.26)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->openai-whisper) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->openai-whisper) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->openai-whisper) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->openai-whisper) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->openai-whisper) (2024.2.0)\r\n",
      "Building wheels for collected packages: openai-whisper\r\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=e8b7c3eb6c0905be4ec7d2a1a27b9ef1d91d28f686a0fced113f9941e83096c7\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/32/d2/9a/801b5cc5b2a1af2e280089b71c326711a682fc1d50ea29d0ed\r\n",
      "Successfully built openai-whisper\r\n",
      "Installing collected packages: openai-whisper\r\n",
      "Successfully installed openai-whisper-20250625\r\n",
      "Collecting yt-dlp\r\n",
      "  Downloading yt_dlp-2025.6.30-py3-none-any.whl.metadata (174 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.3/174.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading yt_dlp-2025.6.30-py3-none-any.whl (3.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: yt-dlp\r\n",
      "Successfully installed yt-dlp-2025.6.30\r\n",
      "🔧 Installation des outils de décompression...\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.5/86.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.9/412.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: rarfile in /usr/local/lib/python3.11/dist-packages (4.2)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "unrar is already the newest version (1:6.1.5-1ubuntu0.1).\r\n",
      "0 upgraded, 0 newly installed, 0 to remove and 87 not upgraded.\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install openai-whisper\n",
    "!pip install yt-dlp\n",
    "\n",
    "print(\"🔧 Installation des outils de décompression...\")\n",
    "!pip install -q py7zr rarfile patool\n",
    "!pip install -q librosa soundfile pydub tqdm\n",
    "\n",
    "!pip install rarfile\n",
    "!apt-get install unrar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08920d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T22:19:15.584451Z",
     "iopub.status.busy": "2025-07-08T22:19:15.584217Z",
     "iopub.status.idle": "2025-07-08T22:19:23.623610Z",
     "shell.execute_reply": "2025-07-08T22:19:23.622812Z"
    },
    "papermill": {
     "duration": 8.062142,
     "end_time": "2025-07-08T22:19:23.625050",
     "exception": false,
     "start_time": "2025-07-08T22:19:15.562908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "import whisper\n",
    "import torch\n",
    "\n",
    "import yt_dlp\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import zipfile\n",
    "import tarfile\n",
    "import rarfile\n",
    "import py7zr\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Configuration pour Kaggle\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08307c33",
   "metadata": {
    "papermill": {
     "duration": 0.020348,
     "end_time": "2025-07-08T22:19:23.666614",
     "exception": false,
     "start_time": "2025-07-08T22:19:23.646266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ===============================================\n",
    "# CLASSE EXTRACTEUR DATASET\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba168f76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T22:19:23.710000Z",
     "iopub.status.busy": "2025-07-08T22:19:23.709113Z",
     "iopub.status.idle": "2025-07-08T22:19:23.882113Z",
     "shell.execute_reply": "2025-07-08T22:19:23.881591Z"
    },
    "papermill": {
     "duration": 0.19587,
     "end_time": "2025-07-08T22:19:23.883299",
     "exception": false,
     "start_time": "2025-07-08T22:19:23.687429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FonDatasetExtractor:\n",
    "    \"\"\"\n",
    "    Extracteur et processeur de dataset audio Fon\n",
    "    @hopsyder - Optimisé pour Kaggle\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_path=\"/kaggle/input/fon-audios-raw\", \n",
    "                 output_path=\"/kaggle/working/fon_dataset_extracted\"):\n",
    "        \"\"\"\n",
    "        Initialisation de l'extracteur\n",
    "        \n",
    "        Args:\n",
    "            input_path: Chemin vers le dataset compressé\n",
    "            output_path: Chemin de sortie pour les fichiers extraits\n",
    "        \"\"\"\n",
    "        self.input_path = Path(input_path)\n",
    "        self.output_path = Path(output_path)\n",
    "        self.extracted_files = []\n",
    "        self.audio_files = []\n",
    "        self.metadata = {}\n",
    "        \n",
    "        # Création du dossier de sortie\n",
    "        self.output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"📁 Dossier input: {self.input_path}\")\n",
    "        print(f\"📁 Dossier output: {self.output_path}\")\n",
    "        \n",
    "        # Vérification de l'existence du dataset\n",
    "        if not self.input_path.exists():\n",
    "            raise FileNotFoundError(f\"Dataset non trouvé: {self.input_path}\")\n",
    "        \n",
    "        print(\"✅ Extracteur initialisé!\")\n",
    "    \n",
    "    def detect_archive_type(self, file_path):\n",
    "        \"\"\"Détection automatique du type d'archive\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        extension = file_path.suffix.lower()\n",
    "        \n",
    "        archive_types = {\n",
    "            '.zip': 'zip',\n",
    "            '.tar': 'tar',\n",
    "            '.tar.gz': 'tar.gz',\n",
    "            '.tgz': 'tar.gz',\n",
    "            '.tar.bz2': 'tar.bz2',\n",
    "            '.tar.xz': 'tar.xz',\n",
    "            '.rar': 'rar',\n",
    "            '.7z': '7z',\n",
    "            '.gz': 'gz'\n",
    "        }\n",
    "        \n",
    "        # Vérification extension composée\n",
    "        if file_path.name.endswith('.tar.gz'):\n",
    "            return 'tar.gz'\n",
    "        elif file_path.name.endswith('.tar.bz2'):\n",
    "            return 'tar.bz2'\n",
    "        elif file_path.name.endswith('.tar.xz'):\n",
    "            return 'tar.xz'\n",
    "        \n",
    "        return archive_types.get(extension, 'unknown')\n",
    "    \n",
    "    def extract_archive(self, archive_path, extract_to=None):\n",
    "        \"\"\"\n",
    "        Extraction d'archive avec gestion de tous les formats\n",
    "        \n",
    "        Args:\n",
    "            archive_path: Chemin vers l'archive\n",
    "            extract_to: Dossier de destination (optionnel)\n",
    "        \"\"\"\n",
    "        archive_path = Path(archive_path)\n",
    "        extract_to = extract_to or self.output_path\n",
    "        extract_to = Path(extract_to)\n",
    "        \n",
    "        archive_type = self.detect_archive_type(archive_path)\n",
    "        \n",
    "        print(f\"📦 Extraction de {archive_path.name} (type: {archive_type})\")\n",
    "        \n",
    "        try:\n",
    "            if archive_type == 'zip':\n",
    "                with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(extract_to)\n",
    "                    extracted_files = zip_ref.namelist()\n",
    "            \n",
    "            elif archive_type in ['tar', 'tar.gz', 'tar.bz2', 'tar.xz']:\n",
    "                mode_map = {\n",
    "                    'tar': 'r',\n",
    "                    'tar.gz': 'r:gz',\n",
    "                    'tar.bz2': 'r:bz2',\n",
    "                    'tar.xz': 'r:xz'\n",
    "                }\n",
    "                \n",
    "                with tarfile.open(archive_path, mode_map[archive_type]) as tar_ref:\n",
    "                    tar_ref.extractall(extract_to)\n",
    "                    extracted_files = tar_ref.getnames()\n",
    "            \n",
    "            elif archive_type == 'rar':\n",
    "                with rarfile.RarFile(archive_path) as rar_ref:\n",
    "                    rar_ref.extractall(extract_to)\n",
    "                    extracted_files = rar_ref.namelist()\n",
    "            \n",
    "            elif archive_type == '7z':\n",
    "                with py7zr.SevenZipFile(archive_path, mode='r') as z7_ref:\n",
    "                    z7_ref.extractall(extract_to)\n",
    "                    extracted_files = z7_ref.getnames()\n",
    "            \n",
    "            elif archive_type == 'gz':\n",
    "                # Pour les fichiers .gz simples\n",
    "                import gzip\n",
    "                output_file = extract_to / archive_path.stem\n",
    "                with gzip.open(archive_path, 'rb') as gz_file:\n",
    "                    with open(output_file, 'wb') as out_file:\n",
    "                        shutil.copyfileobj(gz_file, out_file)\n",
    "                extracted_files = [output_file.name]\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Type d'archive non supporté: {archive_type}\")\n",
    "            \n",
    "            print(f\"✅ Extraction réussie: {len(extracted_files)} fichiers\")\n",
    "            return extracted_files\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur extraction: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def scan_input_directory(self):\n",
    "        \"\"\"Scan du dossier input pour identifier les archives\"\"\"\n",
    "        print(\"🔍 Scan du dossier input...\")\n",
    "        \n",
    "        archives_found = []\n",
    "        \n",
    "        for item in self.input_path.rglob(\"*\"):\n",
    "            if item.is_file():\n",
    "                archive_type = self.detect_archive_type(item)\n",
    "                if archive_type != 'unknown':\n",
    "                    archives_found.append({\n",
    "                        'path': item,\n",
    "                        'name': item.name,\n",
    "                        'size': item.stat().st_size,\n",
    "                        'type': archive_type\n",
    "                    })\n",
    "        \n",
    "        if not archives_found:\n",
    "            print(\"⚠️ Aucune archive trouvée, recherche de fichiers audio directs...\")\n",
    "            # Recherche de fichiers audio non compressés\n",
    "            audio_extensions = ['.wav', '.mp3', '.m4a', '.flac', '.ogg', '.aac']\n",
    "            for ext in audio_extensions:\n",
    "                audio_files = list(self.input_path.rglob(f\"*{ext}\"))\n",
    "                if audio_files:\n",
    "                    print(f\"📄 Trouvé {len(audio_files)} fichiers {ext}\")\n",
    "                    self.audio_files.extend(audio_files)\n",
    "        \n",
    "        print(f\"📦 Archives trouvées: {len(archives_found)}\")\n",
    "        for archive in archives_found:\n",
    "            size_mb = archive['size'] / (1024 * 1024)\n",
    "            print(f\"   - {archive['name']} ({archive['type']}) - {size_mb:.1f} MB\")\n",
    "        \n",
    "        return archives_found\n",
    "    \n",
    "    def extract_all_archives(self):\n",
    "        \"\"\"Extraction de toutes les archives trouvées\"\"\"\n",
    "        archives = self.scan_input_directory()\n",
    "        \n",
    "        if not archives:\n",
    "            print(\"ℹ️ Aucune archive à extraire\")\n",
    "            return\n",
    "        \n",
    "        total_extracted = 0\n",
    "        \n",
    "        for archive in archives:\n",
    "            print(f\"\\n📦 Traitement: {archive['name']}\")\n",
    "            \n",
    "            # Création d'un sous-dossier pour chaque archive\n",
    "            archive_output_dir = self.output_path / archive['path'].stem\n",
    "            archive_output_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Extraction\n",
    "            extracted_files = self.extract_archive(archive['path'], archive_output_dir)\n",
    "            \n",
    "            if extracted_files:\n",
    "                self.extracted_files.extend(extracted_files)\n",
    "                total_extracted += len(extracted_files)\n",
    "                \n",
    "                # Recherche des fichiers audio dans les extraits\n",
    "                self.find_audio_files_in_directory(archive_output_dir)\n",
    "        \n",
    "        print(f\"\\n✅ Extraction terminée: {total_extracted} fichiers extraits\")\n",
    "        print(f\"🎵 Fichiers audio trouvés: {len(self.audio_files)}\")\n",
    "    \n",
    "    def find_audio_files_in_directory(self, directory):\n",
    "        \"\"\"Recherche récursive des fichiers audio\"\"\"\n",
    "        audio_extensions = ['.wav', '.mp3', '.m4a', '.flac', '.ogg', '.aac', '.wma']\n",
    "        \n",
    "        for ext in audio_extensions:\n",
    "            audio_files = list(Path(directory).rglob(f\"*{ext}\"))\n",
    "            self.audio_files.extend(audio_files)\n",
    "    \n",
    "    def analyze_audio_files(self):\n",
    "        \"\"\"Analyse des fichiers audio extraits\"\"\"\n",
    "        if not self.audio_files:\n",
    "            print(\"⚠️ Aucun fichier audio à analyser\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"🔍 Analyse de {len(self.audio_files)} fichiers audio...\")\n",
    "        \n",
    "        audio_metadata = []\n",
    "        \n",
    "        for audio_file in tqdm(self.audio_files, desc=\"Analyse audio\"):\n",
    "            try:\n",
    "                # Informations de base\n",
    "                file_info = {\n",
    "                    'filename': audio_file.name,\n",
    "                    'filepath': str(audio_file),\n",
    "                    'size_mb': audio_file.stat().st_size / (1024 * 1024),\n",
    "                    'extension': audio_file.suffix.lower()\n",
    "                }\n",
    "                \n",
    "                # Analyse audio avec librosa (échantillonnage pour rapidité)\n",
    "                try:\n",
    "                    # Chargement de seulement 30 secondes pour l'analyse\n",
    "                    y, sr = librosa.load(str(audio_file), duration=30, sr=None)\n",
    "                    \n",
    "                    file_info.update({\n",
    "                        'duration_analyzed': len(y) / sr,\n",
    "                        'sample_rate': sr,\n",
    "                        'channels': 1 if y.ndim == 1 else y.shape[0],\n",
    "                        'rms_energy': float(np.sqrt(np.mean(y**2))),\n",
    "                        'zero_crossing_rate': float(np.mean(librosa.feature.zero_crossing_rate(y)[0])),\n",
    "                        'spectral_centroid': float(np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)[0]))\n",
    "                    })\n",
    "                    \n",
    "                    # Estimation de la durée totale via pydub (plus rapide)\n",
    "                    audio_segment = AudioSegment.from_file(str(audio_file))\n",
    "                    file_info['total_duration'] = len(audio_segment) / 1000.0  # en secondes\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    file_info.update({\n",
    "                        'error': str(e),\n",
    "                        'analysis_status': 'failed'\n",
    "                    })\n",
    "                \n",
    "                audio_metadata.append(file_info)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Erreur analyse {audio_file.name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Création du DataFrame\n",
    "        metadata_df = pd.DataFrame(audio_metadata)\n",
    "        \n",
    "        # Statistiques\n",
    "        if not metadata_df.empty:\n",
    "            print(f\"\\n📊 Statistiques du dataset:\")\n",
    "            print(f\"   - Nombre de fichiers: {len(metadata_df)}\")\n",
    "            print(f\"   - Taille totale: {metadata_df['size_mb'].sum():.1f} MB\")\n",
    "            print(f\"   - Durée totale: {metadata_df['total_duration'].sum()/3600:.1f} heures\")\n",
    "            print(f\"   - Formats: {metadata_df['extension'].value_counts().to_dict()}\")\n",
    "            print(f\"   - Sample rates: {metadata_df['sample_rate'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return metadata_df\n",
    "    \n",
    "    def create_dataset_structure(self):\n",
    "        \"\"\"Création d'une structure de dataset organisée\"\"\"\n",
    "        print(\"🏗️ Création de la structure de dataset...\")\n",
    "        \n",
    "        # Dossiers organisés\n",
    "        structured_path = self.output_path / \"structured_dataset\"\n",
    "        \n",
    "        folders = {\n",
    "            'raw_audio': structured_path / \"raw_audio\",\n",
    "            'processed_audio': structured_path / \"processed_audio\", \n",
    "            'metadata': structured_path / \"metadata\",\n",
    "            'transcriptions': structured_path / \"transcriptions\",\n",
    "            'analysis': structured_path / \"analysis\"\n",
    "        }\n",
    "        \n",
    "        # Création des dossiers\n",
    "        for folder_name, folder_path in folders.items():\n",
    "            folder_path.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"   📁 {folder_name}: {folder_path}\")\n",
    "        \n",
    "        # Copie des fichiers audio vers raw_audio\n",
    "        if self.audio_files:\n",
    "            print(f\"📋 Copie de {len(self.audio_files)} fichiers audio...\")\n",
    "            \n",
    "            for audio_file in tqdm(self.audio_files[:50], desc=\"Copie fichiers\"):  # Limite pour Kaggle\n",
    "                try:\n",
    "                    dest_file = folders['raw_audio'] / audio_file.name\n",
    "                    shutil.copy2(audio_file, dest_file)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Erreur copie {audio_file.name}: {e}\")\n",
    "        \n",
    "        self.structured_path = structured_path\n",
    "        return folders\n",
    "    \n",
    "    def save_metadata_and_analysis(self, metadata_df):\n",
    "        \"\"\"Sauvegarde des métadonnées et analyses\"\"\"\n",
    "        if metadata_df is None or metadata_df.empty:\n",
    "            print(\"⚠️ Pas de métadonnées à sauvegarder\")\n",
    "            return\n",
    "        \n",
    "        # Sauvegarde CSV\n",
    "        metadata_file = self.output_path / \"fon_dataset_metadata.csv\"\n",
    "        metadata_df.to_csv(metadata_file, index=False)\n",
    "        print(f\"💾 Métadonnées sauvées: {metadata_file}\")\n",
    "        \n",
    "        # Sauvegarde JSON détaillée\n",
    "        json_file = self.output_path / \"fon_dataset_analysis.json\"\n",
    "        analysis_data = {\n",
    "            'extraction_info': {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'total_files': len(self.audio_files),\n",
    "                'total_size_mb': metadata_df['size_mb'].sum(),\n",
    "                'total_duration_hours': metadata_df['total_duration'].sum() / 3600\n",
    "            },\n",
    "            'file_statistics': {\n",
    "                'formats': metadata_df['extension'].value_counts().to_dict(),\n",
    "                'sample_rates': metadata_df['sample_rate'].value_counts().to_dict(),\n",
    "                'avg_duration': metadata_df['total_duration'].mean(),\n",
    "                'avg_size_mb': metadata_df['size_mb'].mean()\n",
    "            },\n",
    "            'quality_metrics': {\n",
    "                'avg_rms_energy': metadata_df['rms_energy'].mean(),\n",
    "                'avg_spectral_centroid': metadata_df['spectral_centroid'].mean(),\n",
    "                'files_with_errors': len(metadata_df[metadata_df['analysis_status'] == 'failed'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(analysis_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"💾 Analyse JSON sauvée: {json_file}\")\n",
    "        \n",
    "        # Visualisations\n",
    "        self.create_visualizations(metadata_df)\n",
    "    \n",
    "    def create_visualizations(self, metadata_df):\n",
    "        \"\"\"Création de visualisations du dataset\"\"\"\n",
    "        print(\"📊 Génération des visualisations...\")\n",
    "        \n",
    "        # Configuration\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Fon Dataset Analysis - @hopsyder', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Distribution des durées\n",
    "        axes[0, 0].hist(metadata_df['total_duration'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].set_title('Distribution des Durées')\n",
    "        axes[0, 0].set_xlabel('Durée (secondes)')\n",
    "        axes[0, 0].set_ylabel('Nombre de fichiers')\n",
    "        \n",
    "        # 2. Distribution des tailles\n",
    "        axes[0, 1].hist(metadata_df['size_mb'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        axes[0, 1].set_title('Distribution des Tailles')\n",
    "        axes[0, 1].set_xlabel('Taille (MB)')\n",
    "        axes[0, 1].set_ylabel('Nombre de fichiers')\n",
    "        \n",
    "        # 3. Formats de fichiers\n",
    "        format_counts = metadata_df['extension'].value_counts()\n",
    "        axes[1, 0].pie(format_counts.values, labels=format_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1, 0].set_title('Répartition des Formats')\n",
    "        \n",
    "        # 4. Sample rates\n",
    "        sr_counts = metadata_df['sample_rate'].value_counts()\n",
    "        axes[1, 1].bar(sr_counts.index.astype(str), sr_counts.values, color='orange', alpha=0.7)\n",
    "        axes[1, 1].set_title('Sample Rates')\n",
    "        axes[1, 1].set_xlabel('Sample Rate (Hz)')\n",
    "        axes[1, 1].set_ylabel('Nombre de fichiers')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Sauvegarde\n",
    "        viz_file = self.output_path / \"fon_dataset_visualization.png\"\n",
    "        plt.savefig(viz_file, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"📊 Visualisation sauvée: {viz_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534eac7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T22:19:23.926751Z",
     "iopub.status.busy": "2025-07-08T22:19:23.926516Z",
     "iopub.status.idle": "2025-07-08T22:19:23.933005Z",
     "shell.execute_reply": "2025-07-08T22:19:23.932338Z"
    },
    "papermill": {
     "duration": 0.02935,
     "end_time": "2025-07-08T22:19:23.934097",
     "exception": false,
     "start_time": "2025-07-08T22:19:23.904747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# FONCTION PRINCIPALE D'EXTRACTION\n",
    "# ===============================================\n",
    "\n",
    "def extract_fon_dataset():\n",
    "    \"\"\"\n",
    "    Fonction principale d'extraction et traitement du dataset Fon\n",
    "    @hopsyder\n",
    "    \"\"\"\n",
    "    print(\"🎬 === EXTRACTION DATASET FON AUDIO ===\")\n",
    "    print(f\"📅 Début: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialisation de l'extracteur\n",
    "        extractor = FonDatasetExtractor()\n",
    "        \n",
    "        # Extraction des archives\n",
    "        print(\"\\n📦 PHASE 1: EXTRACTION DES ARCHIVES\")\n",
    "        extractor.extract_all_archives()\n",
    "        \n",
    "        # Si pas d'archives, recherche directe des fichiers audio\n",
    "        if not extractor.audio_files:\n",
    "            print(\"\\n🔍 PHASE 1b: RECHERCHE DIRECTE FICHIERS AUDIO\")\n",
    "            extractor.find_audio_files_in_directory(extractor.input_path)\n",
    "        \n",
    "        # Analyse des fichiers audio\n",
    "        print(\"\\n🎵 PHASE 2: ANALYSE DES FICHIERS AUDIO\")\n",
    "        metadata_df = extractor.analyze_audio_files()\n",
    "        \n",
    "        # Création de la structure organisée\n",
    "        print(\"\\n🏗️ PHASE 3: STRUCTURATION DU DATASET\")\n",
    "        folders = extractor.create_dataset_structure()\n",
    "        \n",
    "        # Sauvegarde des métadonnées\n",
    "        print(\"\\n💾 PHASE 4: SAUVEGARDE MÉTADONNÉES\")\n",
    "        extractor.save_metadata_and_analysis(metadata_df)\n",
    "        \n",
    "        # Résumé final\n",
    "        print(\"\\n🎉 === EXTRACTION TERMINÉE ===\")\n",
    "        if metadata_df is not None and not metadata_df.empty:\n",
    "            print(f\"📊 RÉSUMÉ:\")\n",
    "            print(f\"   - Fichiers extraits: {len(extractor.audio_files)}\")\n",
    "            print(f\"   - Taille totale: {metadata_df['size_mb'].sum():.1f} MB\")\n",
    "            print(f\"   - Durée totale: {metadata_df['total_duration'].sum()/3600:.2f} heures\")\n",
    "            print(f\"   - Formats: {metadata_df['extension'].value_counts().to_dict()}\")\n",
    "            \n",
    "            # Fichiers de sortie\n",
    "            print(f\"\\n📁 FICHIERS GÉNÉRÉS:\")\n",
    "            print(f\"   - Dataset structuré: /kaggle/working/fon_dataset_extracted/structured_dataset/\")\n",
    "            print(f\"   - Métadonnées: fon_dataset_metadata.csv\")\n",
    "            print(f\"   - Analyse: fon_dataset_analysis.json\")\n",
    "            print(f\"   - Visualisation: fon_dataset_visualization.png\")\n",
    "            \n",
    "            return metadata_df, extractor\n",
    "        else:\n",
    "            print(\"⚠️ Aucun fichier audio traité\")\n",
    "            return None, extractor\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERREUR CRITIQUE: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "    \n",
    "    finally:\n",
    "        print(f\"\\n⏱️ Fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b8753b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T22:19:23.975811Z",
     "iopub.status.busy": "2025-07-08T22:19:23.975380Z",
     "iopub.status.idle": "2025-07-08T22:19:24.037590Z",
     "shell.execute_reply": "2025-07-08T22:19:24.036715Z"
    },
    "papermill": {
     "duration": 0.084014,
     "end_time": "2025-07-08T22:19:24.038611",
     "exception": false,
     "start_time": "2025-07-08T22:19:23.954597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎬 === EXTRACTION DATASET FON AUDIO ===\n",
      "📅 Début: 2025-07-08 22:19:23\n",
      "📁 Dossier input: /kaggle/input/fon-audios-raw\n",
      "📁 Dossier output: /kaggle/working/fon_dataset_extracted\n",
      "✅ Extracteur initialisé!\n",
      "\n",
      "📦 PHASE 1: EXTRACTION DES ARCHIVES\n",
      "🔍 Scan du dossier input...\n",
      "⚠️ Aucune archive trouvée, recherche de fichiers audio directs...\n",
      "📦 Archives trouvées: 0\n",
      "ℹ️ Aucune archive à extraire\n",
      "\n",
      "🔍 PHASE 1b: RECHERCHE DIRECTE FICHIERS AUDIO\n",
      "\n",
      "🎵 PHASE 2: ANALYSE DES FICHIERS AUDIO\n",
      "⚠️ Aucun fichier audio à analyser\n",
      "\n",
      "🏗️ PHASE 3: STRUCTURATION DU DATASET\n",
      "🏗️ Création de la structure de dataset...\n",
      "   📁 raw_audio: /kaggle/working/fon_dataset_extracted/structured_dataset/raw_audio\n",
      "   📁 processed_audio: /kaggle/working/fon_dataset_extracted/structured_dataset/processed_audio\n",
      "   📁 metadata: /kaggle/working/fon_dataset_extracted/structured_dataset/metadata\n",
      "   📁 transcriptions: /kaggle/working/fon_dataset_extracted/structured_dataset/transcriptions\n",
      "   📁 analysis: /kaggle/working/fon_dataset_extracted/structured_dataset/analysis\n",
      "\n",
      "💾 PHASE 4: SAUVEGARDE MÉTADONNÉES\n",
      "⚠️ Pas de métadonnées à sauvegarder\n",
      "\n",
      "🎉 === EXTRACTION TERMINÉE ===\n",
      "⚠️ Aucun fichier audio traité\n",
      "\n",
      "⏱️ Fin: 2025-07-08 22:19:24\n",
      "\n",
      "🔚 === SCRIPT TERMINÉ ===\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# FONCTION UTILITAIRE POUR KAGGLE DATASET\n",
    "# ===============================================\n",
    "\n",
    "def create_kaggle_dataset_version():\n",
    "    \"\"\"\n",
    "    Crée une nouvelle version du dataset pour votre profil Kaggle\n",
    "    \"\"\"\n",
    "    print(\"📤 Préparation pour upload Kaggle Dataset...\")\n",
    "    \n",
    "    # Vérification des fichiers requis\n",
    "    required_files = [\n",
    "        \"/kaggle/working/fon_dataset_extracted/structured_dataset/\",\n",
    "        \"/kaggle/working/fon_dataset_metadata.csv\",\n",
    "        \"/kaggle/working/fon_dataset_analysis.json\"\n",
    "    ]\n",
    "    \n",
    "    existing_files = [f for f in required_files if os.path.exists(f)]\n",
    "    \n",
    "    print(f\"✅ Fichiers prêts: {len(existing_files)}/{len(required_files)}\")\n",
    "    \n",
    "    # Création du dataset-metadata.json pour Kaggle\n",
    "    kaggle_metadata = {\n",
    "        \"title\": \"Fon Language Audio Dataset - Processed\",\n",
    "        \"id\": \"votre-username/fon-audio-processed\",\n",
    "        \"description\": \"Dataset audio en langue Fon extrait et traité avec métadonnées complètes\",\n",
    "        \"tags\": [\"audio\", \"nlp\", \"african-languages\", \"fon\", \"speech-recognition\"],\n",
    "        \"licenses\": [{\"name\": \"CC0-1.0\"}],\n",
    "        \"collaborators\": [],\n",
    "        \"data\": []\n",
    "    }\n",
    "    \n",
    "    with open(\"/kaggle/working/dataset-metadata.json\", \"w\") as f:\n",
    "        json.dump(kaggle_metadata, f, indent=2)\n",
    "    \n",
    "    print(\"📋 Métadonnées Kaggle créées: dataset-metadata.json\")\n",
    "    print(\"\\n🚀 PROCHAINES ÉTAPES:\")\n",
    "    print(\"1. Téléchargez tous les fichiers de /kaggle/working/\")\n",
    "    print(\"2. Créez un nouveau dataset sur kaggle.com/datasets\")\n",
    "    print(\"3. Uploadez les fichiers avec dataset-metadata.json\")\n",
    "    print(\"4. Publiez votre dataset traité !\")\n",
    "\n",
    "# ===============================================\n",
    "# EXÉCUTION PRINCIPALE\n",
    "# ===============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Exécution de l'extraction\n",
    "    metadata_df, extractor = extract_fon_dataset()\n",
    "    \n",
    "    # Préparation pour Kaggle Dataset\n",
    "    if metadata_df is not None:\n",
    "        create_kaggle_dataset_version()\n",
    "    \n",
    "    print(\"\\n🔚 === SCRIPT TERMINÉ ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e6c23",
   "metadata": {
    "papermill": {
     "duration": 0.021996,
     "end_time": "2025-07-08T22:19:24.081453",
     "exception": false,
     "start_time": "2025-07-08T22:19:24.059457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ===============================================\n",
    "# CLASSE PRINCIPALE - PIPELINE TRANSCRIPTION FON\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced704d",
   "metadata": {
    "papermill": {
     "duration": 0.020739,
     "end_time": "2025-07-08T22:19:24.166903",
     "exception": false,
     "start_time": "2025-07-08T22:19:24.146164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12664178,
     "datasetId": 7606064,
     "sourceId": 12131729,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 118.406465,
   "end_time": "2025-07-08T22:19:26.336439",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-08T22:17:27.929974",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
