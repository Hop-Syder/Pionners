{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc90a25d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-08T22:17:32.538343Z",
     "iopub.status.busy": "2025-07-08T22:17:32.538115Z",
     "iopub.status.idle": "2025-07-08T22:17:34.103687Z",
     "shell.execute_reply": "2025-07-08T22:17:34.102888Z"
    },
    "papermill": {
     "duration": 1.570894,
     "end_time": "2025-07-08T22:17:34.105014",
     "exception": false,
     "start_time": "2025-07-08T22:17:32.534120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fon-audios-raw/train-ag-00009-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ac-00005-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-aa-00006-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-p-00004-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-bg-00010-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-s-00003-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-bc-00005-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ah-00010-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ab-00002-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-aq-00009-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-b-00002-of-00003.parquet\n",
      "/kaggle/input/fon-audios-raw/train-i-00005-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ap-00006-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-n-00002-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-k-00004-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-z-00007-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-bf-00011-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-e-00007-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ae-00001-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-a-00001-of-00003.parquet\n",
      "/kaggle/input/fon-audios-raw/train-j-00000-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-u-00008-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-be-00009-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-af-00003-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-an-00003-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-o-00000-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-bd-00008-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-as-00007-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-h-00002-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-d-00008-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-c-00000-of-00003.parquet\n",
      "/kaggle/input/fon-audios-raw/train-l-00009-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ar-00008-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ba-00006-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ax-00000-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-am-00005-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-az-00004-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-g-00003-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-r-00006-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-aw-00003-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ad-00008-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-au-00002-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ao-00004-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-aj-00001-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-w-00007-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-x-00000-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-t-00005-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-f-00001-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-v-00009-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-y-00004-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ai-00011-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-bb-00007-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-at-00010-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ay-00001-of-00012.parquet\n",
      "/kaggle/input/fon-audios-raw/train-q-00001-of-00010.parquet\n",
      "/kaggle/input/fon-audios-raw/train-ak-00000-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-al-00002-of-00011.parquet\n",
      "/kaggle/input/fon-audios-raw/train-m-00006-of-00010.parquet\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb03c74d",
   "metadata": {
    "papermill": {
     "duration": 0.002335,
     "end_time": "2025-07-08T22:17:34.110415",
     "exception": false,
     "start_time": "2025-07-08T22:17:34.108080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# Academi Pionners 3 : Test Pipeline Transcription Fon\n",
    "\n",
    "**@hopsyder - Kaggle Free Tier Optimized**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cfa290",
   "metadata": {
    "papermill": {
     "duration": 0.002292,
     "end_time": "2025-07-08T22:17:34.115154",
     "exception": false,
     "start_time": "2025-07-08T22:17:34.112862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ===============================================\n",
    "# INSTALLATION ET CONFIGURATION INITIALE\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536474c2",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2025-07-08T22:17:34.121507Z",
     "iopub.status.busy": "2025-07-08T22:17:34.120941Z",
     "iopub.status.idle": "2025-07-08T22:19:15.539844Z",
     "shell.execute_reply": "2025-07-08T22:19:15.539124Z"
    },
    "papermill": {
     "duration": 101.423503,
     "end_time": "2025-07-08T22:19:15.541080",
     "exception": false,
     "start_time": "2025-07-08T22:17:34.117577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n",
      "Collecting openai-whisper\r\n",
      "  Downloading openai_whisper-20250625.tar.gz (803 kB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\r\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.26.4)\r\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\r\n",
      "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2.4.1)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.13.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.4.26)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->openai-whisper) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->openai-whisper) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->openai-whisper) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->openai-whisper) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->openai-whisper) (2024.2.0)\r\n",
      "Building wheels for collected packages: openai-whisper\r\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=e8b7c3eb6c0905be4ec7d2a1a27b9ef1d91d28f686a0fced113f9941e83096c7\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/32/d2/9a/801b5cc5b2a1af2e280089b71c326711a682fc1d50ea29d0ed\r\n",
      "Successfully built openai-whisper\r\n",
      "Installing collected packages: openai-whisper\r\n",
      "Successfully installed openai-whisper-20250625\r\n",
      "Collecting yt-dlp\r\n",
      "  Downloading yt_dlp-2025.6.30-py3-none-any.whl.metadata (174 kB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m174.3/174.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading yt_dlp-2025.6.30-py3-none-any.whl (3.3 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: yt-dlp\r\n",
      "Successfully installed yt-dlp-2025.6.30\r\n",
      "ğŸ”§ Installation des outils de dÃ©compression...\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.5/86.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m412.9/412.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: rarfile in /usr/local/lib/python3.11/dist-packages (4.2)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "unrar is already the newest version (1:6.1.5-1ubuntu0.1).\r\n",
      "0 upgraded, 0 newly installed, 0 to remove and 87 not upgraded.\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install openai-whisper\n",
    "!pip install yt-dlp\n",
    "\n",
    "print(\"ğŸ”§ Installation des outils de dÃ©compression...\")\n",
    "!pip install -q py7zr rarfile patool\n",
    "!pip install -q librosa soundfile pydub tqdm\n",
    "\n",
    "!pip install rarfile\n",
    "!apt-get install unrar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08920d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T22:19:15.584451Z",
     "iopub.status.busy": "2025-07-08T22:19:15.584217Z",
     "iopub.status.idle": "2025-07-08T22:19:23.623610Z",
     "shell.execute_reply": "2025-07-08T22:19:23.622812Z"
    },
    "papermill": {
     "duration": 8.062142,
     "end_time": "2025-07-08T22:19:23.625050",
     "exception": false,
     "start_time": "2025-07-08T22:19:15.562908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "import whisper\n",
    "import torch\n",
    "\n",
    "import yt_dlp\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import zipfile\n",
    "import tarfile\n",
    "import rarfile\n",
    "import py7zr\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Configuration pour Kaggle\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08307c33",
   "metadata": {
    "papermill": {
     "duration": 0.020348,
     "end_time": "2025-07-08T22:19:23.666614",
     "exception": false,
     "start_time": "2025-07-08T22:19:23.646266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ===============================================\n",
    "# CLASSE EXTRACTEUR DATASET\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba168f76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T22:19:23.710000Z",
     "iopub.status.busy": "2025-07-08T22:19:23.709113Z",
     "iopub.status.idle": "2025-07-08T22:19:23.882113Z",
     "shell.execute_reply": "2025-07-08T22:19:23.881591Z"
    },
    "papermill": {
     "duration": 0.19587,
     "end_time": "2025-07-08T22:19:23.883299",
     "exception": false,
     "start_time": "2025-07-08T22:19:23.687429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FonDatasetExtractor:\n",
    "    \"\"\"\n",
    "    Extracteur et processeur de dataset audio Fon\n",
    "    @hopsyder - OptimisÃ© pour Kaggle\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_path=\"/kaggle/input/fon-audios-raw\", \n",
    "                 output_path=\"/kaggle/working/fon_dataset_extracted\"):\n",
    "        \"\"\"\n",
    "        Initialisation de l'extracteur\n",
    "        \n",
    "        Args:\n",
    "            input_path: Chemin vers le dataset compressÃ©\n",
    "            output_path: Chemin de sortie pour les fichiers extraits\n",
    "        \"\"\"\n",
    "        self.input_path = Path(input_path)\n",
    "        self.output_path = Path(output_path)\n",
    "        self.extracted_files = []\n",
    "        self.audio_files = []\n",
    "        self.metadata = {}\n",
    "        \n",
    "        # CrÃ©ation du dossier de sortie\n",
    "        self.output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"ğŸ“ Dossier input: {self.input_path}\")\n",
    "        print(f\"ğŸ“ Dossier output: {self.output_path}\")\n",
    "        \n",
    "        # VÃ©rification de l'existence du dataset\n",
    "        if not self.input_path.exists():\n",
    "            raise FileNotFoundError(f\"Dataset non trouvÃ©: {self.input_path}\")\n",
    "        \n",
    "        print(\"âœ… Extracteur initialisÃ©!\")\n",
    "    \n",
    "    def detect_archive_type(self, file_path):\n",
    "        \"\"\"DÃ©tection automatique du type d'archive\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        extension = file_path.suffix.lower()\n",
    "        \n",
    "        archive_types = {\n",
    "            '.zip': 'zip',\n",
    "            '.tar': 'tar',\n",
    "            '.tar.gz': 'tar.gz',\n",
    "            '.tgz': 'tar.gz',\n",
    "            '.tar.bz2': 'tar.bz2',\n",
    "            '.tar.xz': 'tar.xz',\n",
    "            '.rar': 'rar',\n",
    "            '.7z': '7z',\n",
    "            '.gz': 'gz'\n",
    "        }\n",
    "        \n",
    "        # VÃ©rification extension composÃ©e\n",
    "        if file_path.name.endswith('.tar.gz'):\n",
    "            return 'tar.gz'\n",
    "        elif file_path.name.endswith('.tar.bz2'):\n",
    "            return 'tar.bz2'\n",
    "        elif file_path.name.endswith('.tar.xz'):\n",
    "            return 'tar.xz'\n",
    "        \n",
    "        return archive_types.get(extension, 'unknown')\n",
    "    \n",
    "    def extract_archive(self, archive_path, extract_to=None):\n",
    "        \"\"\"\n",
    "        Extraction d'archive avec gestion de tous les formats\n",
    "        \n",
    "        Args:\n",
    "            archive_path: Chemin vers l'archive\n",
    "            extract_to: Dossier de destination (optionnel)\n",
    "        \"\"\"\n",
    "        archive_path = Path(archive_path)\n",
    "        extract_to = extract_to or self.output_path\n",
    "        extract_to = Path(extract_to)\n",
    "        \n",
    "        archive_type = self.detect_archive_type(archive_path)\n",
    "        \n",
    "        print(f\"ğŸ“¦ Extraction de {archive_path.name} (type: {archive_type})\")\n",
    "        \n",
    "        try:\n",
    "            if archive_type == 'zip':\n",
    "                with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(extract_to)\n",
    "                    extracted_files = zip_ref.namelist()\n",
    "            \n",
    "            elif archive_type in ['tar', 'tar.gz', 'tar.bz2', 'tar.xz']:\n",
    "                mode_map = {\n",
    "                    'tar': 'r',\n",
    "                    'tar.gz': 'r:gz',\n",
    "                    'tar.bz2': 'r:bz2',\n",
    "                    'tar.xz': 'r:xz'\n",
    "                }\n",
    "                \n",
    "                with tarfile.open(archive_path, mode_map[archive_type]) as tar_ref:\n",
    "                    tar_ref.extractall(extract_to)\n",
    "                    extracted_files = tar_ref.getnames()\n",
    "            \n",
    "            elif archive_type == 'rar':\n",
    "                with rarfile.RarFile(archive_path) as rar_ref:\n",
    "                    rar_ref.extractall(extract_to)\n",
    "                    extracted_files = rar_ref.namelist()\n",
    "            \n",
    "            elif archive_type == '7z':\n",
    "                with py7zr.SevenZipFile(archive_path, mode='r') as z7_ref:\n",
    "                    z7_ref.extractall(extract_to)\n",
    "                    extracted_files = z7_ref.getnames()\n",
    "            \n",
    "            elif archive_type == 'gz':\n",
    "                # Pour les fichiers .gz simples\n",
    "                import gzip\n",
    "                output_file = extract_to / archive_path.stem\n",
    "                with gzip.open(archive_path, 'rb') as gz_file:\n",
    "                    with open(output_file, 'wb') as out_file:\n",
    "                        shutil.copyfileobj(gz_file, out_file)\n",
    "                extracted_files = [output_file.name]\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Type d'archive non supportÃ©: {archive_type}\")\n",
    "            \n",
    "            print(f\"âœ… Extraction rÃ©ussie: {len(extracted_files)} fichiers\")\n",
    "            return extracted_files\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur extraction: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def scan_input_directory(self):\n",
    "        \"\"\"Scan du dossier input pour identifier les archives\"\"\"\n",
    "        print(\"ğŸ” Scan du dossier input...\")\n",
    "        \n",
    "        archives_found = []\n",
    "        \n",
    "        for item in self.input_path.rglob(\"*\"):\n",
    "            if item.is_file():\n",
    "                archive_type = self.detect_archive_type(item)\n",
    "                if archive_type != 'unknown':\n",
    "                    archives_found.append({\n",
    "                        'path': item,\n",
    "                        'name': item.name,\n",
    "                        'size': item.stat().st_size,\n",
    "                        'type': archive_type\n",
    "                    })\n",
    "        \n",
    "        if not archives_found:\n",
    "            print(\"âš ï¸ Aucune archive trouvÃ©e, recherche de fichiers audio directs...\")\n",
    "            # Recherche de fichiers audio non compressÃ©s\n",
    "            audio_extensions = ['.wav', '.mp3', '.m4a', '.flac', '.ogg', '.aac']\n",
    "            for ext in audio_extensions:\n",
    "                audio_files = list(self.input_path.rglob(f\"*{ext}\"))\n",
    "                if audio_files:\n",
    "                    print(f\"ğŸ“„ TrouvÃ© {len(audio_files)} fichiers {ext}\")\n",
    "                    self.audio_files.extend(audio_files)\n",
    "        \n",
    "        print(f\"ğŸ“¦ Archives trouvÃ©es: {len(archives_found)}\")\n",
    "        for archive in archives_found:\n",
    "            size_mb = archive['size'] / (1024 * 1024)\n",
    "            print(f\"   - {archive['name']} ({archive['type']}) - {size_mb:.1f} MB\")\n",
    "        \n",
    "        return archives_found\n",
    "    \n",
    "    def extract_all_archives(self):\n",
    "        \"\"\"Extraction de toutes les archives trouvÃ©es\"\"\"\n",
    "        archives = self.scan_input_directory()\n",
    "        \n",
    "        if not archives:\n",
    "            print(\"â„¹ï¸ Aucune archive Ã  extraire\")\n",
    "            return\n",
    "        \n",
    "        total_extracted = 0\n",
    "        \n",
    "        for archive in archives:\n",
    "            print(f\"\\nğŸ“¦ Traitement: {archive['name']}\")\n",
    "            \n",
    "            # CrÃ©ation d'un sous-dossier pour chaque archive\n",
    "            archive_output_dir = self.output_path / archive['path'].stem\n",
    "            archive_output_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Extraction\n",
    "            extracted_files = self.extract_archive(archive['path'], archive_output_dir)\n",
    "            \n",
    "            if extracted_files:\n",
    "                self.extracted_files.extend(extracted_files)\n",
    "                total_extracted += len(extracted_files)\n",
    "                \n",
    "                # Recherche des fichiers audio dans les extraits\n",
    "                self.find_audio_files_in_directory(archive_output_dir)\n",
    "        \n",
    "        print(f\"\\nâœ… Extraction terminÃ©e: {total_extracted} fichiers extraits\")\n",
    "        print(f\"ğŸµ Fichiers audio trouvÃ©s: {len(self.audio_files)}\")\n",
    "    \n",
    "    def find_audio_files_in_directory(self, directory):\n",
    "        \"\"\"Recherche rÃ©cursive des fichiers audio\"\"\"\n",
    "        audio_extensions = ['.wav', '.mp3', '.m4a', '.flac', '.ogg', '.aac', '.wma']\n",
    "        \n",
    "        for ext in audio_extensions:\n",
    "            audio_files = list(Path(directory).rglob(f\"*{ext}\"))\n",
    "            self.audio_files.extend(audio_files)\n",
    "    \n",
    "    def analyze_audio_files(self):\n",
    "        \"\"\"Analyse des fichiers audio extraits\"\"\"\n",
    "        if not self.audio_files:\n",
    "            print(\"âš ï¸ Aucun fichier audio Ã  analyser\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"ğŸ” Analyse de {len(self.audio_files)} fichiers audio...\")\n",
    "        \n",
    "        audio_metadata = []\n",
    "        \n",
    "        for audio_file in tqdm(self.audio_files, desc=\"Analyse audio\"):\n",
    "            try:\n",
    "                # Informations de base\n",
    "                file_info = {\n",
    "                    'filename': audio_file.name,\n",
    "                    'filepath': str(audio_file),\n",
    "                    'size_mb': audio_file.stat().st_size / (1024 * 1024),\n",
    "                    'extension': audio_file.suffix.lower()\n",
    "                }\n",
    "                \n",
    "                # Analyse audio avec librosa (Ã©chantillonnage pour rapiditÃ©)\n",
    "                try:\n",
    "                    # Chargement de seulement 30 secondes pour l'analyse\n",
    "                    y, sr = librosa.load(str(audio_file), duration=30, sr=None)\n",
    "                    \n",
    "                    file_info.update({\n",
    "                        'duration_analyzed': len(y) / sr,\n",
    "                        'sample_rate': sr,\n",
    "                        'channels': 1 if y.ndim == 1 else y.shape[0],\n",
    "                        'rms_energy': float(np.sqrt(np.mean(y**2))),\n",
    "                        'zero_crossing_rate': float(np.mean(librosa.feature.zero_crossing_rate(y)[0])),\n",
    "                        'spectral_centroid': float(np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)[0]))\n",
    "                    })\n",
    "                    \n",
    "                    # Estimation de la durÃ©e totale via pydub (plus rapide)\n",
    "                    audio_segment = AudioSegment.from_file(str(audio_file))\n",
    "                    file_info['total_duration'] = len(audio_segment) / 1000.0  # en secondes\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    file_info.update({\n",
    "                        'error': str(e),\n",
    "                        'analysis_status': 'failed'\n",
    "                    })\n",
    "                \n",
    "                audio_metadata.append(file_info)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Erreur analyse {audio_file.name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # CrÃ©ation du DataFrame\n",
    "        metadata_df = pd.DataFrame(audio_metadata)\n",
    "        \n",
    "        # Statistiques\n",
    "        if not metadata_df.empty:\n",
    "            print(f\"\\nğŸ“Š Statistiques du dataset:\")\n",
    "            print(f\"   - Nombre de fichiers: {len(metadata_df)}\")\n",
    "            print(f\"   - Taille totale: {metadata_df['size_mb'].sum():.1f} MB\")\n",
    "            print(f\"   - DurÃ©e totale: {metadata_df['total_duration'].sum()/3600:.1f} heures\")\n",
    "            print(f\"   - Formats: {metadata_df['extension'].value_counts().to_dict()}\")\n",
    "            print(f\"   - Sample rates: {metadata_df['sample_rate'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return metadata_df\n",
    "    \n",
    "    def create_dataset_structure(self):\n",
    "        \"\"\"CrÃ©ation d'une structure de dataset organisÃ©e\"\"\"\n",
    "        print(\"ğŸ—ï¸ CrÃ©ation de la structure de dataset...\")\n",
    "        \n",
    "        # Dossiers organisÃ©s\n",
    "        structured_path = self.output_path / \"structured_dataset\"\n",
    "        \n",
    "        folders = {\n",
    "            'raw_audio': structured_path / \"raw_audio\",\n",
    "            'processed_audio': structured_path / \"processed_audio\", \n",
    "            'metadata': structured_path / \"metadata\",\n",
    "            'transcriptions': structured_path / \"transcriptions\",\n",
    "            'analysis': structured_path / \"analysis\"\n",
    "        }\n",
    "        \n",
    "        # CrÃ©ation des dossiers\n",
    "        for folder_name, folder_path in folders.items():\n",
    "            folder_path.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"   ğŸ“ {folder_name}: {folder_path}\")\n",
    "        \n",
    "        # Copie des fichiers audio vers raw_audio\n",
    "        if self.audio_files:\n",
    "            print(f\"ğŸ“‹ Copie de {len(self.audio_files)} fichiers audio...\")\n",
    "            \n",
    "            for audio_file in tqdm(self.audio_files[:50], desc=\"Copie fichiers\"):  # Limite pour Kaggle\n",
    "                try:\n",
    "                    dest_file = folders['raw_audio'] / audio_file.name\n",
    "                    shutil.copy2(audio_file, dest_file)\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Erreur copie {audio_file.name}: {e}\")\n",
    "        \n",
    "        self.structured_path = structured_path\n",
    "        return folders\n",
    "    \n",
    "    def save_metadata_and_analysis(self, metadata_df):\n",
    "        \"\"\"Sauvegarde des mÃ©tadonnÃ©es et analyses\"\"\"\n",
    "        if metadata_df is None or metadata_df.empty:\n",
    "            print(\"âš ï¸ Pas de mÃ©tadonnÃ©es Ã  sauvegarder\")\n",
    "            return\n",
    "        \n",
    "        # Sauvegarde CSV\n",
    "        metadata_file = self.output_path / \"fon_dataset_metadata.csv\"\n",
    "        metadata_df.to_csv(metadata_file, index=False)\n",
    "        print(f\"ğŸ’¾ MÃ©tadonnÃ©es sauvÃ©es: {metadata_file}\")\n",
    "        \n",
    "        # Sauvegarde JSON dÃ©taillÃ©e\n",
    "        json_file = self.output_path / \"fon_dataset_analysis.json\"\n",
    "        analysis_data = {\n",
    "            'extraction_info': {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'total_files': len(self.audio_files),\n",
    "                'total_size_mb': metadata_df['size_mb'].sum(),\n",
    "                'total_duration_hours': metadata_df['total_duration'].sum() / 3600\n",
    "            },\n",
    "            'file_statistics': {\n",
    "                'formats': metadata_df['extension'].value_counts().to_dict(),\n",
    "                'sample_rates': metadata_df['sample_rate'].value_counts().to_dict(),\n",
    "                'avg_duration': metadata_df['total_duration'].mean(),\n",
    "                'avg_size_mb': metadata_df['size_mb'].mean()\n",
    "            },\n",
    "            'quality_metrics': {\n",
    "                'avg_rms_energy': metadata_df['rms_energy'].mean(),\n",
    "                'avg_spectral_centroid': metadata_df['spectral_centroid'].mean(),\n",
    "                'files_with_errors': len(metadata_df[metadata_df['analysis_status'] == 'failed'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(analysis_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Analyse JSON sauvÃ©e: {json_file}\")\n",
    "        \n",
    "        # Visualisations\n",
    "        self.create_visualizations(metadata_df)\n",
    "    \n",
    "    def create_visualizations(self, metadata_df):\n",
    "        \"\"\"CrÃ©ation de visualisations du dataset\"\"\"\n",
    "        print(\"ğŸ“Š GÃ©nÃ©ration des visualisations...\")\n",
    "        \n",
    "        # Configuration\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Fon Dataset Analysis - @hopsyder', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Distribution des durÃ©es\n",
    "        axes[0, 0].hist(metadata_df['total_duration'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].set_title('Distribution des DurÃ©es')\n",
    "        axes[0, 0].set_xlabel('DurÃ©e (secondes)')\n",
    "        axes[0, 0].set_ylabel('Nombre de fichiers')\n",
    "        \n",
    "        # 2. Distribution des tailles\n",
    "        axes[0, 1].hist(metadata_df['size_mb'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        axes[0, 1].set_title('Distribution des Tailles')\n",
    "        axes[0, 1].set_xlabel('Taille (MB)')\n",
    "        axes[0, 1].set_ylabel('Nombre de fichiers')\n",
    "        \n",
    "        # 3. Formats de fichiers\n",
    "        format_counts = metadata_df['extension'].value_counts()\n",
    "        axes[1, 0].pie(format_counts.values, labels=format_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1, 0].set_title('RÃ©partition des Formats')\n",
    "        \n",
    "        # 4. Sample rates\n",
    "        sr_counts = metadata_df['sample_rate'].value_counts()\n",
    "        axes[1, 1].bar(sr_counts.index.astype(str), sr_counts.values, color='orange', alpha=0.7)\n",
    "        axes[1, 1].set_title('Sample Rates')\n",
    "        axes[1, 1].set_xlabel('Sample Rate (Hz)')\n",
    "        axes[1, 1].set_ylabel('Nombre de fichiers')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Sauvegarde\n",
    "        viz_file = self.output_path / \"fon_dataset_visualization.png\"\n",
    "        plt.savefig(viz_file, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"ğŸ“Š Visualisation sauvÃ©e: {viz_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534eac7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T22:19:23.926751Z",
     "iopub.status.busy": "2025-07-08T22:19:23.926516Z",
     "iopub.status.idle": "2025-07-08T22:19:23.933005Z",
     "shell.execute_reply": "2025-07-08T22:19:23.932338Z"
    },
    "papermill": {
     "duration": 0.02935,
     "end_time": "2025-07-08T22:19:23.934097",
     "exception": false,
     "start_time": "2025-07-08T22:19:23.904747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# FONCTION PRINCIPALE D'EXTRACTION\n",
    "# ===============================================\n",
    "\n",
    "def extract_fon_dataset():\n",
    "    \"\"\"\n",
    "    Fonction principale d'extraction et traitement du dataset Fon\n",
    "    @hopsyder\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¬ === EXTRACTION DATASET FON AUDIO ===\")\n",
    "    print(f\"ğŸ“… DÃ©but: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialisation de l'extracteur\n",
    "        extractor = FonDatasetExtractor()\n",
    "        \n",
    "        # Extraction des archives\n",
    "        print(\"\\nğŸ“¦ PHASE 1: EXTRACTION DES ARCHIVES\")\n",
    "        extractor.extract_all_archives()\n",
    "        \n",
    "        # Si pas d'archives, recherche directe des fichiers audio\n",
    "        if not extractor.audio_files:\n",
    "            print(\"\\nğŸ” PHASE 1b: RECHERCHE DIRECTE FICHIERS AUDIO\")\n",
    "            extractor.find_audio_files_in_directory(extractor.input_path)\n",
    "        \n",
    "        # Analyse des fichiers audio\n",
    "        print(\"\\nğŸµ PHASE 2: ANALYSE DES FICHIERS AUDIO\")\n",
    "        metadata_df = extractor.analyze_audio_files()\n",
    "        \n",
    "        # CrÃ©ation de la structure organisÃ©e\n",
    "        print(\"\\nğŸ—ï¸ PHASE 3: STRUCTURATION DU DATASET\")\n",
    "        folders = extractor.create_dataset_structure()\n",
    "        \n",
    "        # Sauvegarde des mÃ©tadonnÃ©es\n",
    "        print(\"\\nğŸ’¾ PHASE 4: SAUVEGARDE MÃ‰TADONNÃ‰ES\")\n",
    "        extractor.save_metadata_and_analysis(metadata_df)\n",
    "        \n",
    "        # RÃ©sumÃ© final\n",
    "        print(\"\\nğŸ‰ === EXTRACTION TERMINÃ‰E ===\")\n",
    "        if metadata_df is not None and not metadata_df.empty:\n",
    "            print(f\"ğŸ“Š RÃ‰SUMÃ‰:\")\n",
    "            print(f\"   - Fichiers extraits: {len(extractor.audio_files)}\")\n",
    "            print(f\"   - Taille totale: {metadata_df['size_mb'].sum():.1f} MB\")\n",
    "            print(f\"   - DurÃ©e totale: {metadata_df['total_duration'].sum()/3600:.2f} heures\")\n",
    "            print(f\"   - Formats: {metadata_df['extension'].value_counts().to_dict()}\")\n",
    "            \n",
    "            # Fichiers de sortie\n",
    "            print(f\"\\nğŸ“ FICHIERS GÃ‰NÃ‰RÃ‰S:\")\n",
    "            print(f\"   - Dataset structurÃ©: /kaggle/working/fon_dataset_extracted/structured_dataset/\")\n",
    "            print(f\"   - MÃ©tadonnÃ©es: fon_dataset_metadata.csv\")\n",
    "            print(f\"   - Analyse: fon_dataset_analysis.json\")\n",
    "            print(f\"   - Visualisation: fon_dataset_visualization.png\")\n",
    "            \n",
    "            return metadata_df, extractor\n",
    "        else:\n",
    "            print(\"âš ï¸ Aucun fichier audio traitÃ©\")\n",
    "            return None, extractor\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERREUR CRITIQUE: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "    \n",
    "    finally:\n",
    "        print(f\"\\nâ±ï¸ Fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b8753b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T22:19:23.975811Z",
     "iopub.status.busy": "2025-07-08T22:19:23.975380Z",
     "iopub.status.idle": "2025-07-08T22:19:24.037590Z",
     "shell.execute_reply": "2025-07-08T22:19:24.036715Z"
    },
    "papermill": {
     "duration": 0.084014,
     "end_time": "2025-07-08T22:19:24.038611",
     "exception": false,
     "start_time": "2025-07-08T22:19:23.954597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¬ === EXTRACTION DATASET FON AUDIO ===\n",
      "ğŸ“… DÃ©but: 2025-07-08 22:19:23\n",
      "ğŸ“ Dossier input: /kaggle/input/fon-audios-raw\n",
      "ğŸ“ Dossier output: /kaggle/working/fon_dataset_extracted\n",
      "âœ… Extracteur initialisÃ©!\n",
      "\n",
      "ğŸ“¦ PHASE 1: EXTRACTION DES ARCHIVES\n",
      "ğŸ” Scan du dossier input...\n",
      "âš ï¸ Aucune archive trouvÃ©e, recherche de fichiers audio directs...\n",
      "ğŸ“¦ Archives trouvÃ©es: 0\n",
      "â„¹ï¸ Aucune archive Ã  extraire\n",
      "\n",
      "ğŸ” PHASE 1b: RECHERCHE DIRECTE FICHIERS AUDIO\n",
      "\n",
      "ğŸµ PHASE 2: ANALYSE DES FICHIERS AUDIO\n",
      "âš ï¸ Aucun fichier audio Ã  analyser\n",
      "\n",
      "ğŸ—ï¸ PHASE 3: STRUCTURATION DU DATASET\n",
      "ğŸ—ï¸ CrÃ©ation de la structure de dataset...\n",
      "   ğŸ“ raw_audio: /kaggle/working/fon_dataset_extracted/structured_dataset/raw_audio\n",
      "   ğŸ“ processed_audio: /kaggle/working/fon_dataset_extracted/structured_dataset/processed_audio\n",
      "   ğŸ“ metadata: /kaggle/working/fon_dataset_extracted/structured_dataset/metadata\n",
      "   ğŸ“ transcriptions: /kaggle/working/fon_dataset_extracted/structured_dataset/transcriptions\n",
      "   ğŸ“ analysis: /kaggle/working/fon_dataset_extracted/structured_dataset/analysis\n",
      "\n",
      "ğŸ’¾ PHASE 4: SAUVEGARDE MÃ‰TADONNÃ‰ES\n",
      "âš ï¸ Pas de mÃ©tadonnÃ©es Ã  sauvegarder\n",
      "\n",
      "ğŸ‰ === EXTRACTION TERMINÃ‰E ===\n",
      "âš ï¸ Aucun fichier audio traitÃ©\n",
      "\n",
      "â±ï¸ Fin: 2025-07-08 22:19:24\n",
      "\n",
      "ğŸ”š === SCRIPT TERMINÃ‰ ===\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# FONCTION UTILITAIRE POUR KAGGLE DATASET\n",
    "# ===============================================\n",
    "\n",
    "def create_kaggle_dataset_version():\n",
    "    \"\"\"\n",
    "    CrÃ©e une nouvelle version du dataset pour votre profil Kaggle\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“¤ PrÃ©paration pour upload Kaggle Dataset...\")\n",
    "    \n",
    "    # VÃ©rification des fichiers requis\n",
    "    required_files = [\n",
    "        \"/kaggle/working/fon_dataset_extracted/structured_dataset/\",\n",
    "        \"/kaggle/working/fon_dataset_metadata.csv\",\n",
    "        \"/kaggle/working/fon_dataset_analysis.json\"\n",
    "    ]\n",
    "    \n",
    "    existing_files = [f for f in required_files if os.path.exists(f)]\n",
    "    \n",
    "    print(f\"âœ… Fichiers prÃªts: {len(existing_files)}/{len(required_files)}\")\n",
    "    \n",
    "    # CrÃ©ation du dataset-metadata.json pour Kaggle\n",
    "    kaggle_metadata = {\n",
    "        \"title\": \"Fon Language Audio Dataset - Processed\",\n",
    "        \"id\": \"votre-username/fon-audio-processed\",\n",
    "        \"description\": \"Dataset audio en langue Fon extrait et traitÃ© avec mÃ©tadonnÃ©es complÃ¨tes\",\n",
    "        \"tags\": [\"audio\", \"nlp\", \"african-languages\", \"fon\", \"speech-recognition\"],\n",
    "        \"licenses\": [{\"name\": \"CC0-1.0\"}],\n",
    "        \"collaborators\": [],\n",
    "        \"data\": []\n",
    "    }\n",
    "    \n",
    "    with open(\"/kaggle/working/dataset-metadata.json\", \"w\") as f:\n",
    "        json.dump(kaggle_metadata, f, indent=2)\n",
    "    \n",
    "    print(\"ğŸ“‹ MÃ©tadonnÃ©es Kaggle crÃ©Ã©es: dataset-metadata.json\")\n",
    "    print(\"\\nğŸš€ PROCHAINES Ã‰TAPES:\")\n",
    "    print(\"1. TÃ©lÃ©chargez tous les fichiers de /kaggle/working/\")\n",
    "    print(\"2. CrÃ©ez un nouveau dataset sur kaggle.com/datasets\")\n",
    "    print(\"3. Uploadez les fichiers avec dataset-metadata.json\")\n",
    "    print(\"4. Publiez votre dataset traitÃ© !\")\n",
    "\n",
    "# ===============================================\n",
    "# EXÃ‰CUTION PRINCIPALE\n",
    "# ===============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ExÃ©cution de l'extraction\n",
    "    metadata_df, extractor = extract_fon_dataset()\n",
    "    \n",
    "    # PrÃ©paration pour Kaggle Dataset\n",
    "    if metadata_df is not None:\n",
    "        create_kaggle_dataset_version()\n",
    "    \n",
    "    print(\"\\nğŸ”š === SCRIPT TERMINÃ‰ ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e6c23",
   "metadata": {
    "papermill": {
     "duration": 0.021996,
     "end_time": "2025-07-08T22:19:24.081453",
     "exception": false,
     "start_time": "2025-07-08T22:19:24.059457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ===============================================\n",
    "# CLASSE PRINCIPALE - PIPELINE TRANSCRIPTION FON\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced704d",
   "metadata": {
    "papermill": {
     "duration": 0.020739,
     "end_time": "2025-07-08T22:19:24.166903",
     "exception": false,
     "start_time": "2025-07-08T22:19:24.146164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12664178,
     "datasetId": 7606064,
     "sourceId": 12131729,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 118.406465,
   "end_time": "2025-07-08T22:19:26.336439",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-08T22:17:27.929974",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
